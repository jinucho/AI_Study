# Fine-tuning Project

μ΄ λ””λ ‰ν† λ¦¬λ” **Qwen3-8B λ¨λΈ**μ„ ν™μ©ν• **M&A(μΈμν•©λ³‘) λ¶„μ•Ό νΉν™” νμΈνλ‹** ν”„λ΅μ νΈμ…λ‹λ‹¤. AI Hub λ°μ΄ν„°μ…‹κ³Ό μμ²΄ ν¬λ΅¤λ§ λ°μ΄ν„°λ¥Ό ν™μ©ν•μ—¬ κΈμµ, λ²•λ¥ , M&A λ¶„μ•Ό μ „λ¬Έ AI λ¨λΈμ„ κµ¬μ¶•ν•©λ‹λ‹¤.

## π“ ν”„λ΅μ νΈ κµ¬μ΅°

```
Fine_tuning/
β”β”€β”€ README.md                           # μ΄ νμΌ (ν”„λ΅μ νΈ κ°€μ΄λ“)
β””β”€β”€ notebook/                          # Jupyter λ…ΈνΈλ¶ λ””λ ‰ν† λ¦¬
    β”β”€β”€ qwen3_finetune.ipynb           # Qwen3-8B λ¨λΈ νμΈνλ‹ λ©”μΈ λ…ΈνΈλ¶
    β”β”€β”€ ai_hub_data_preocessing.ipynb  # AI Hub λ°μ΄ν„°μ…‹ μ „μ²λ¦¬ λ…ΈνΈλ¶
    β””β”€β”€ assets/                        # ν•™μµ κ²°κ³Ό μ΄λ―Έμ§€
        β”β”€β”€ train.png                  # ν•™μµ κ³Όμ • μ‹κ°ν™”
        β””β”€β”€ system.png                 # μ‹μ¤ν… κµ¬μ„±λ„
```

## π― ν”„λ΅μ νΈ λ©ν‘

1. **λ„λ©”μΈ νΉν™” λ¨λΈ κµ¬μ¶•**: M&A, κΈμµ, λ²•λ¥  λ¶„μ•Ό μ „λ¬Έ μ§€μ‹ ν•™μµ
2. **ν•κµ­μ–΄ μµμ ν™”**: ν•κµ­μ–΄ μ§μμ‘λ‹µ μ„±λ¥ ν–¥μƒ
3. **μ‹¤μ©μ  ν™μ©**: μ‹¤μ  μ—…λ¬΄μ— ν™μ© κ°€λ¥ν• μμ¤€μ λ¨λΈ κ°λ°
4. **ν¨μ¨μ  ν•™μµ**: LoRA κΈ°λ²•μ„ ν†µν• ν¨μ¨μ μΈ νμΈνλ‹

## π”§ κΈ°μ  μ¤νƒ

### λ¨λΈ & ν”„λ μ„μ›ν¬
- **Base Model**: Qwen3-8B (Alibaba Cloud)
- **Fine-tuning**: LoRA (Low-Rank Adaptation)
- **Framework**: Transformers, PEFT, TRL
- **Monitoring**: Weights & Biases (WandB)

### λ°μ΄ν„°
- **AI Hub**: ν•κµ­μ–΄ μ§μμ‘λ‹µ λ°μ΄ν„°μ…‹
- **μμ²΄ λ°μ΄ν„°**: M&A κ΄€λ ¨ ν¬λ΅¤λ§ λ°μ΄ν„°
- **μ „μ²λ¦¬**: ν’μ§ ν•„ν„°λ§, μΉ΄ν…κ³ λ¦¬ λ¶„λ¥

### ν™κ²½
- **Python**: 3.10.16
- **GPU**: CUDA μ§€μ› ν™κ²½
- **Storage**: λ΅μ»¬ PC κΈ°λ° ν•™μµ

## π“ λ°μ΄ν„°μ…‹ μ •λ³΄

### 1. AI Hub λ°μ΄ν„°μ…‹
- **μ¶μ²**: [AI Hub ν•κµ­μ–΄ λ§λ­‰μΉ λ°μ΄ν„°](https://www.aihub.or.kr/aihubdata/data/view.do?currMenu=&topMenu=&aihubDataSe=data&dataSetSn=71748)
- **κµ¬μ„±**: Training, Validation, Other λ°μ΄ν„°
- **μ „μ²λ¦¬**: ν’μ§ μ μ 30μ  μ΄μƒ ν•„ν„°λ§ (9,605κ° μƒν”)
- **μΉ΄ν…κ³ λ¦¬**: μΈλ¬Έ, μ‚¬ν, μμ—°κ³Όν•™ λ“± (μμ²΄λ¥, μΆ…κµ, λ³΄κ±΄ μ μ™Έ)

### 2. M&A μ „μ© λ°μ΄ν„°
- **ν•νƒ**: JSON ν•μ‹μ μ§μμ‘λ‹µ μ
- **λ‚΄μ©**: M&A κ΄€λ ¨ μ „λ¬Έ μ§€μ‹, λ²•λ¥  μ •λ³΄, μ μ°¨ μ•λ‚΄
- **νΉμ§•**: λ„λ©”μΈ νΉν™” κ³ ν’μ§ λ°μ΄ν„°

### 3. λ°μ΄ν„° ν†µν•©
- **μ΄ μƒν” μ**: μ•½ 14,000κ° (AI Hub + M&A λ°μ΄ν„°)
- **ν•μ‹**: Question-Answer μ
- **μ–Έμ–΄**: ν•κµ­μ–΄ μ „μ©
- **ν’μ§ κ΄€λ¦¬**: μ¤‘λ³µ μ κ±°, ν•μ‹ ν†µμΌ

## π€ μ‚¬μ©λ²•

### 1. ν™κ²½ μ„¤μ •

```bash
# ν•„μ” λΌμ΄λΈλ¬λ¦¬ μ„¤μΉ
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118
pip install transformers datasets accelerate peft bitsandbytes trl wandb huggingface_hub
```

### 2. λ°μ΄ν„° μ „μ²λ¦¬

```bash
# AI Hub λ°μ΄ν„° μ „μ²λ¦¬ μ‹¤ν–‰
jupyter notebook ai_hub_data_preocessing.ipynb
```

**μ£Όμ” μ „μ²λ¦¬ κ³Όμ •**:
- AI Hub λ°μ΄ν„°μ…‹ μ••μ¶• ν•΄μ  λ° νμ‹±
- ν’μ§ μ μ κΈ°λ° ν•„ν„°λ§ (score > 30)
- μΉ΄ν…κ³ λ¦¬λ³„ λ¶„λ¥ λ° λ¶ν•„μ” μΉ΄ν…κ³ λ¦¬ μ κ±°
- M&A λ°μ΄ν„°μ™€ ν†µν•©
- μ¤‘λ³µ λ°μ΄ν„° μ κ±°

### 3. λ¨λΈ νμΈνλ‹

```bash
# νμΈνλ‹ μ‹¤ν–‰
jupyter notebook qwen3_finetune.ipynb
```

**μ£Όμ” ν•™μµ κ³Όμ •**:
1. **λ°μ΄ν„° λ΅λ“**: μ „μ²λ¦¬λ QA λ°μ΄ν„°μ…‹ λ΅λ“
2. **ν† ν¬λ‚μ΄μ € μ„¤μ •**: Qwen3 μ „μ© ν† ν¬λ‚μ΄μ € κµ¬μ„±
3. **λ¨λΈ λ΅λ“**: Qwen3-8B λ² μ΄μ¤ λ¨λΈ λ΅λ“
4. **LoRA μ„¤μ •**: ν¨μ¨μ  νμΈνλ‹μ„ μ„ν• LoRA κµ¬μ„±
5. **ν•™μµ μ‹¤ν–‰**: SFTTrainerλ¥Ό ν†µν• μ§€λ„ ν•™μµ
6. **λ¨λΈ μ €μ¥**: νμΈνλ‹λ λ¨λΈ μ €μ¥

## β™οΈ ν•µμ‹¬ μ„¤μ •

### LoRA κµ¬μ„±
```python
config = LoraConfig(
    task_type=TaskType.CAUSAL_LM,
    target_modules=["q_proj", "k_proj", "v_proj", "o_proj", 
                   "gate_proj", "up_proj", "down_proj"],
    r=8,                    # LoRA rank
    lora_alpha=32,          # LoRA alpha
    lora_dropout=0.1,       # Dropout rate
    inference_mode=False    # Training mode
)
```

### ν•™μµ νλΌλ―Έν„°
```python
training_args = TrainingArguments(
    output_dir="./models/Qwen3-8B-mna-Finetuned",
    num_train_epochs=3,
    per_device_train_batch_size=1,
    gradient_accumulation_steps=8,
    learning_rate=1e-4,
    max_grad_norm=0.3,
    warmup_ratio=0.03,
    lr_scheduler_type="linear"
)
```

### ν”„λ΅¬ν”„νΈ ν…ν”λ¦Ώ
```
<|im_start|>system
λ‹Ήμ‹ μ€ κΈμµ,λ²•λ¥  λ° M&A λ¶„μ•Ό μ „λ¬Έκ°€μ…λ‹λ‹¤.
μ‚¬μ©μ μ§λ¬Έμ— λ€ν•΄ λ°λ“μ‹ ν•κµ­μ–΄λ΅λ§ λ‹µλ³€ν•μ„Έμ”.
<|im_end|>
<|im_start|>user
{question}/no_think
<|im_end|>
<|im_start|>assistant
<think>

</think>

{answer}
<|im_end|>
```

## π“ ν•™μµ λ¨λ‹ν„°λ§

### WandB ν†µν•©
- **ν”„λ΅μ νΈλ…**: Qwen3-8B-M&A
- **μ‹¤ν—λ…**: qlora_finetuning-qwen3-8B-mna
- **μ¶”μ  μ§€ν‘**: Loss, Learning Rate, Gradient Norm

### μ €μ¥ μ „λµ
- **μ²΄ν¬ν¬μΈνΈ**: 10 μ¤ν…λ§λ‹¤ μ €μ¥
- **λ΅κΉ…**: 10 μ¤ν…λ§λ‹¤ λ΅κ·Έ κΈ°λ΅
- **μµμΆ… λ¨λΈ**: `./models/Qwen3-8B-mna-Finetuned`

## π” λ¨λΈ ν‰κ°€

### ν…μ¤νΈ ν•¨μ
```python
def generate_answer(question):
    # μ§λ¬Έμ— λ€ν• λ‹µλ³€ μƒμ„±
    # μ‹μ¤ν… ν”„λ΅¬ν”„νΈμ™€ μ‚¬μ©μ μ§λ¬Έμ„ ν…ν”λ¦Ώμ— μ μ©
    # λ¨λΈ μ¶”λ΅  μ‹¤ν–‰
    return generated_answer
```

### ν‰κ°€ κΈ°μ¤€
- **λ„λ©”μΈ μ •ν™•μ„±**: M&A κ΄€λ ¨ μ§λ¬Έμ— λ€ν• μ •ν™•ν• λ‹µλ³€
- **μ–Έμ–΄ ν’μ§**: μμ—°μ¤λ¬μ΄ ν•κµ­μ–΄ μƒμ„±
- **μΌκ΄€μ„±**: λ™μΌν• μ§λ¬Έμ— λ€ν• μΌκ΄€λ λ‹µλ³€
- **μ „λ¬Έμ„±**: κΈμµ, λ²•λ¥  μ©μ–΄μ μ μ ν• μ‚¬μ©

## π’΅ μ£Όμ” νΉμ§•

### 1. λ„λ©”μΈ νΉν™”
- M&A, κΈμµ, λ²•λ¥  λ¶„μ•Ό μ „λ¬Έ μ§€μ‹ ν•™μµ
- μ—…κ³„ μ©μ–΄μ™€ κ°λ…μ— λ€ν• μ •ν™•ν• μ΄ν•΄
- μ‹¤λ¬΄μ§„μ΄ ν™μ© κ°€λ¥ν• μμ¤€μ λ‹µλ³€ ν’μ§

### 2. ν•κµ­μ–΄ μµμ ν™”
- ν•κµ­μ–΄ μ§μμ‘λ‹µμ— νΉν™”λ ν•™μµ
- ν•κµ­μ λ²•λ¥ , μ λ„μ  λ§¥λ½ λ°μ
- μμ—°μ¤λ¬μ΄ ν•κµ­μ–΄ ν‘ν„ μƒμ„±

### 3. ν¨μ¨μ  ν•™μµ
- LoRA κΈ°λ²•μΌλ΅ λ©”λ¨λ¦¬ ν¨μ¨μ„± ν™•λ³΄
- λ΅μ»¬ PC ν™κ²½μ—μ„ ν•™μµ κ°€λ¥
- λΉ λ¥Έ μλ ΄κ³Ό μ•μ •μ μΈ ν•™μµ

### 4. μ‹¤μ©μ  ν™μ©
- μ‹¤μ  μ—…λ¬΄ ν™κ²½μ—μ„ ν™μ© κ°€λ¥
- API ν•νƒλ΅ μ„λΉ„μ¤ λ°°ν¬ κ°€λ¥
- μ§€μ†μ μΈ λ¨λΈ κ°μ„  κ°€λ¥

## π”§ λ¬Έμ  ν•΄κ²°

### μμ£Ό λ°μƒν•λ” λ¬Έμ 

1. **λ©”λ¨λ¦¬ λ¶€μ΅±**
   - λ°°μΉ ν¬κΈ° μ΅°μ •: `per_device_train_batch_size=1`
   - Gradient Accumulation ν™μ©: `gradient_accumulation_steps=8`

2. **CUDA μ¤λ¥**
   - PyTorch CUDA λ²„μ „ ν™•μΈ
   - GPU λ©”λ¨λ¦¬ μ •λ¦¬: `torch.cuda.empty_cache()`

3. **ν† ν¬λ‚μ΄μ € μ¤λ¥**
   - `use_fast=False` μ„¤μ •
   - `trust_remote_code=True` μ„¤μ •

4. **WandB μ—°κ²° μ¤λ¥**
   - API ν‚¤ μ¬μ„¤μ •: `wandb login`
   - ν”„λ΅μ‹ μ„¤μ • ν™•μΈ

### λ””λ²„κΉ… ν
- μ‘μ€ λ°μ΄ν„°μ…‹μΌλ΅ λ¨Όμ € ν…μ¤νΈ
- ν•™μµλ¥  μ΅°μ •μΌλ΅ μλ ΄ κ°μ„ 
- μ •κΈ°μ μΈ μ²΄ν¬ν¬μΈνΈ μ €μ¥

## π“ μ°Έκ³  μλ£

### μ°Έκ³  λ¬Έμ„
- [Qwen3 λ¨λΈ λ¬Έμ„](https://huggingface.co/Qwen/Qwen3-8B)
- [Self-LLM Qwen2 νν† λ¦¬μ–Ό](https://github.com/datawhalechina/self-llm/blob/master/models/Qwen2/05-Qwen2-7B-Instruct%20Lora.ipynb)